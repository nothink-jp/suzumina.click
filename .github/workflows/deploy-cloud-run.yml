name: Deploy to Cloud Run

on:
  push:
    branches:
      - main
    paths:
      - 'apps/web/**'
      - 'packages/shared-types/**'
      - 'packages/ui/**'
      - '.github/workflows/deploy-cloud-run.yml'
  
  # Manual triggering
  workflow_dispatch:

env:
  PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
  REGION: asia-northeast1
  SERVICE_NAME: suzumina-click-web
  REPOSITORY: suzumina-click-web
  IMAGE_NAME: web

jobs:
  deploy:
    runs-on: ubuntu-latest
    
    permissions:
      contents: read
      id-token: write
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Enable Corepack
        run: corepack enable
      
      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          run_install: false
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '22'
          cache: 'pnpm'
      
      - name: Get pnpm store directory
        shell: bash
        run: |
          echo "STORE_PATH=$(pnpm store path --silent)" >> $GITHUB_ENV
      
      - name: Setup pnpm cache
        uses: actions/cache@v4
        with:
          path: ${{ env.STORE_PATH }}
          key: ${{ runner.os }}-pnpm-store-${{ hashFiles('**/pnpm-lock.yaml') }}
          restore-keys: |
            ${{ runner.os }}-pnpm-store-
      
      - name: Install dependencies (optimized)
        run: |
          # Install with optimized pnpm settings for CI
          pnpm install --frozen-lockfile
      
      # Cache Next.js build cache for faster rebuilds
      - name: Cache Next.js build
        uses: actions/cache@v4
        with:
          path: |
            apps/web/.next/cache
          key: ${{ runner.os }}-nextjs-${{ hashFiles('apps/web/package.json', 'pnpm-lock.yaml') }}-${{ hashFiles('apps/web/**/*.ts', 'apps/web/**/*.tsx', 'apps/web/**/*.js', 'apps/web/**/*.jsx') }}
          restore-keys: |
            ${{ runner.os }}-nextjs-${{ hashFiles('apps/web/package.json', 'pnpm-lock.yaml') }}-
            ${{ runner.os }}-nextjs-
      
      # Parallel build of shared packages
      - name: Build shared packages (parallel)
        run: |
          # Build shared-types and UI in parallel when possible
          pnpm --filter @suzumina.click/shared-types build &
          SHARED_TYPES_PID=$!
          
          # UI doesn't need build, but we can prepare if needed
          echo "UI package ready (no build needed)"
          
          # Wait for shared-types to complete
          wait $SHARED_TYPES_PID
          echo "‚úÖ Shared packages built successfully"
      
      # Run quality checks in parallel for faster feedback
      - name: Run quality checks (parallel)
        run: |
          echo "üß™ Running tests, linting, and type checking in parallel..."
          
          # Run tests in background
          pnpm --filter web test &
          TEST_PID=$!
          
          # Run linting in background  
          pnpm --filter web lint &
          LINT_PID=$!
          
          # Run type checking in background
          pnpm --filter web typecheck &
          TYPECHECK_PID=$!
          
          # Wait for all processes to complete
          wait $TEST_PID && echo "‚úÖ Tests passed" || { echo "‚ùå Tests failed"; exit 1; }
          wait $LINT_PID && echo "‚úÖ Linting passed" || { echo "‚ùå Linting failed"; exit 1; }
          wait $TYPECHECK_PID && echo "‚úÖ Type checking passed" || { echo "‚ùå Type checking failed"; exit 1; }
          
          echo "üéâ All quality checks completed successfully"
      
      # Google Cloud authentication
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: 'projects/${{ secrets.GCP_PROJECT_NUMBER }}/locations/global/workloadIdentityPools/github-pool/providers/github-provider'
          service_account: 'github-actions-sa@${{ secrets.GCP_PROJECT_ID }}.iam.gserviceaccount.com'
      
      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.PROJECT_ID }}
      
      - name: Configure Docker to use gcloud as a credential helper
        run: |
          gcloud auth configure-docker ${{ env.REGION }}-docker.pkg.dev
      
      # Set up Docker Buildx for advanced caching
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      # Build and push Docker image with advanced caching
      - name: Build and push Docker image (optimized)
        run: |
          # Generate image tag
          IMAGE_TAG="${{ env.REGION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}"
          LATEST_TAG="${{ env.REGION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.IMAGE_NAME }}:latest"
          CACHE_TAG="${{ env.REGION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.IMAGE_NAME }}:cache"
          
          # Build with cache optimization and multi-platform support
          docker buildx build \
            --platform linux/amd64 \
            --file apps/web/Dockerfile \
            --tag $IMAGE_TAG \
            --tag $LATEST_TAG \
            --cache-from type=registry,ref=$CACHE_TAG \
            --cache-to type=registry,ref=$CACHE_TAG,mode=max \
            --push \
            .
          
          echo "‚úÖ Docker image built and pushed with optimized caching"
          
          # Immediate cache cleanup after build
          echo "üßπ Cleaning up build cache tags..."
          
          # Check if there are multiple cache tags and delete older ones
          CACHE_IMAGES=$(gcloud artifacts docker images list \
            ${{ env.REGION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.IMAGE_NAME }} \
            --filter="tags:cache" \
            --format="value(IMAGE@DIGEST,createTime)" \
            --sort-by="~createTime" | tail -n +2)
          
          if [ -n "$CACHE_IMAGES" ]; then
            echo "Found old cache images to clean up"
            echo "$CACHE_IMAGES" | while IFS=$'\t' read -r digest createTime; do
              if [ -n "$digest" ]; then
                echo "Removing old cache image: $digest (created: $createTime)"
                gcloud artifacts docker images delete "$digest" --quiet || echo "Failed to delete $digest"
              fi
            done
          else
            echo "No old cache images to clean up"
          fi
          
          # Save image tag for deployment
          echo "IMAGE_TAG=$IMAGE_TAG" >> $GITHUB_ENV
      
      # Deploy to Cloud Run
      - name: Deploy to Cloud Run
        run: |
          gcloud run deploy ${{ env.SERVICE_NAME }} \
            --image ${{ env.IMAGE_TAG }} \
            --region ${{ env.REGION }} \
            --platform managed \
            --allow-unauthenticated \
            --service-account="cloud-run-nextjs@${{ env.PROJECT_ID }}.iam.gserviceaccount.com" \
            --set-env-vars="NODE_ENV=production,NEXT_TELEMETRY_DISABLED=1,GOOGLE_CLOUD_PROJECT=${{ env.PROJECT_ID }},NEXTAUTH_URL=https://suzumina.click,AUTH_TRUST_HOST=true,NEXT_PUBLIC_GA_MEASUREMENT_ID=G-9SYZ48LBPH,NEXT_PUBLIC_GTM_ID=GTM-W7QT5PCR,NEXT_PUBLIC_ADSENSE_CLIENT_ID=ca-pub-8077945848616354" \
            --set-secrets="DISCORD_CLIENT_ID=DISCORD_CLIENT_ID:latest,DISCORD_CLIENT_SECRET=DISCORD_CLIENT_SECRET:latest,NEXTAUTH_SECRET=NEXTAUTH_SECRET:latest,YOUTUBE_API_KEY=YOUTUBE_API_KEY:latest" \
            --memory 2Gi \
            --cpu 1 \
            --min-instances 0 \
            --max-instances 10 \
            --timeout 300 \
            --port 8080

      # Ensure public access is properly configured
      - name: Configure public access
        run: |
          echo "Configuring public access for Cloud Run service..."
          
          # Remove any existing IAM bindings first to ensure clean state
          gcloud run services remove-iam-policy-binding ${{ env.SERVICE_NAME }} \
            --region=${{ env.REGION }} \
            --member="allUsers" \
            --role="roles/run.invoker" \
            --quiet || echo "No existing policy binding to remove"
          
          # Add IAM policy binding for public access
          gcloud run services add-iam-policy-binding ${{ env.SERVICE_NAME }} \
            --region=${{ env.REGION }} \
            --member="allUsers" \
            --role="roles/run.invoker"
          
          # Verify the policy was applied
          echo "Verifying IAM policy..."
          gcloud run services get-iam-policy ${{ env.SERVICE_NAME }} \
            --region=${{ env.REGION }} \
            --format="yaml"
      
      # Health check
      - name: Wait for deployment and health check
        run: |
          # Get service URL (Cloud Run v2 format)
          SERVICE_URL=$(gcloud run services describe ${{ env.SERVICE_NAME }} \
            --region ${{ env.REGION }} \
            --format 'value(status.address.url)')
          
          echo "Service URL: $SERVICE_URL"
          
          # Fallback if v2 format doesn't work
          if [ -z "$SERVICE_URL" ]; then
            echo "Trying v1 format..."
            SERVICE_URL=$(gcloud run services describe ${{ env.SERVICE_NAME }} \
              --region ${{ env.REGION }} \
              --format 'value(status.url)')
            echo "V1 Service URL: $SERVICE_URL"
          fi
          
          # Wait for deployment to be ready and IAM to propagate
          echo "Waiting for deployment and IAM policy propagation..."
          sleep 60
          
          # Health check with more retries and better error handling
          # Use custom domain if available, otherwise use service URL with bypassed host check
          HEALTH_URL="$SERVICE_URL/api/health"
          if [ "${{ github.ref }}" = "refs/heads/main" ]; then
            echo "Using custom domain for health check..."
            HEALTH_URL="https://suzumina.click/api/health"
          fi
          
          echo "Health check URL: $HEALTH_URL"
          
          for i in {1..10}; do
            echo "Health check attempt $i/10..."
            
            # If using service URL, add Host header to bypass middleware restriction
            if [[ "$HEALTH_URL" == "$SERVICE_URL"* ]]; then
              HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" -H "Host: suzumina.click" "$HEALTH_URL" || echo "000")
            else
              HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" "$HEALTH_URL" || echo "000")
            fi
            
            if [ "$HTTP_CODE" = "200" ]; then
              echo "‚úÖ Health check passed (HTTP $HTTP_CODE)"
              if [[ "$HEALTH_URL" == "$SERVICE_URL"* ]]; then
                curl -s -H "Host: suzumina.click" "$HEALTH_URL" | jq '.' || echo "Response received"
              else
                curl -s "$HEALTH_URL" | jq '.' || echo "Response received"
              fi
              exit 0
            else
              echo "‚ùå Health check failed (HTTP $HTTP_CODE) - attempt $i/10"
              if [ "$HTTP_CODE" = "403" ]; then
                echo "403 Forbidden - IAM policy may still be propagating..."
              elif [ "$HTTP_CODE" = "404" ]; then
                echo "404 Not Found - service may still be starting or host blocked..."
              elif [ "$HTTP_CODE" = "000" ]; then
                echo "Connection failed - service may be unavailable..."
              fi
              sleep 15
            fi
          done
          
          echo "‚ùå Health check failed after 10 attempts"
          echo "Final attempt details:"
          if [[ "$HEALTH_URL" == "$SERVICE_URL"* ]]; then
            curl -v -H "Host: suzumina.click" "$HEALTH_URL" || true
          else
            curl -v "$HEALTH_URL" || true
          fi
          exit 1
      
      # Cleanup old Docker images and Cloud Run revisions
      - name: Cleanup old images and revisions
        run: |
          echo "üßπ Starting cleanup of old Docker images and Cloud Run revisions..."
          
          KEEP_IMAGES=5
          KEEP_REVISIONS=3
          REPOSITORY_PATH="${{ env.REGION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.IMAGE_NAME }}"
          
          # 1. Cloud Run revision cleanup
          echo "üóÇÔ∏è  Cleaning up Cloud Run revisions (keeping latest $KEEP_REVISIONS)..."
          
          # Get all revisions sorted by creation time (newest first)
          REVISIONS=$(gcloud run revisions list \
            --service="${{ env.SERVICE_NAME }}" \
            --region="${{ env.REGION }}" \
            --sort-by="~metadata.creationTimestamp" \
            --format="value(metadata.name)" \
            --limit=50)
          
          if [ -n "$REVISIONS" ]; then
            REVISION_ARRAY=($REVISIONS)
            TOTAL_REVISIONS=${#REVISION_ARRAY[@]}
            echo "Found $TOTAL_REVISIONS revisions"
            
            if [ $TOTAL_REVISIONS -gt $KEEP_REVISIONS ]; then
              DELETE_COUNT=$((TOTAL_REVISIONS - KEEP_REVISIONS))
              echo "Deleting $DELETE_COUNT old revisions..."
              
              for ((i=KEEP_REVISIONS; i<TOTAL_REVISIONS; i++)); do
                REVISION=${REVISION_ARRAY[$i]}
                echo "  Deleting revision: $REVISION"
                
                if gcloud run revisions delete "$REVISION" \
                  --region="${{ env.REGION }}" \
                  --quiet 2>/dev/null; then
                  echo "    ‚úÖ Deleted successfully"
                else
                  echo "    ‚ö†Ô∏è  Failed to delete (may be in use)"
                fi
              done
            else
              echo "‚úÖ Revision count is within limit ($TOTAL_REVISIONS revisions)"
            fi
          else
            echo "No revisions found"
          fi
          
          # 2. Docker image cleanup
          echo "üê≥ Cleaning up Docker images (keeping latest $KEEP_IMAGES)..."
          
          # Get all image digests sorted by creation time (oldest first for deletion)
          IMAGES=$(gcloud artifacts docker images list "$REPOSITORY_PATH" \
            --sort-by="CREATE_TIME" \
            --format="value(IMAGE)" \
            --limit=100 2>/dev/null || echo "")
          
          if [ -n "$IMAGES" ]; then
            IMAGE_ARRAY=($IMAGES)
            TOTAL_IMAGES=${#IMAGE_ARRAY[@]}
            echo "Found $TOTAL_IMAGES images"
            
            if [ $TOTAL_IMAGES -gt $KEEP_IMAGES ]; then
              DELETE_COUNT=$((TOTAL_IMAGES - KEEP_IMAGES))
              echo "Deleting $DELETE_COUNT old images..."
              
              # Delete from beginning (oldest images) when sorted by CREATE_TIME ascending
              for ((i=0; i<DELETE_COUNT; i++)); do
                IMAGE=${IMAGE_ARRAY[$i]}
                echo "  Deleting image: $IMAGE"
                
                if gcloud artifacts docker images delete "$IMAGE" \
                  --delete-tags \
                  --quiet 2>/dev/null; then
                  echo "    ‚úÖ Deleted successfully"
                else
                  echo "    ‚ö†Ô∏è  Failed to delete (may be in use)"
                fi
              done
            else
              echo "‚úÖ Image count is within limit ($TOTAL_IMAGES images)"
            fi
          else
            echo "No images found"
          fi
          
          # 3. Final status
          echo "üìä Cleanup summary:"
          
          # Count remaining images
          REMAINING_IMAGES=$(gcloud artifacts docker images list "$REPOSITORY_PATH" \
            --format="value(IMAGE)" 2>/dev/null | wc -l || echo "0")
          echo "  Docker images: $REMAINING_IMAGES remaining"
          
          # Count remaining revisions
          REMAINING_REVISIONS=$(gcloud run revisions list \
            --service="${{ env.SERVICE_NAME }}" \
            --region="${{ env.REGION }}" \
            --format="value(metadata.name)" | wc -l || echo "0")
          echo "  Cloud Run revisions: $REMAINING_REVISIONS remaining"
          
          echo "üéâ Cleanup completed!"